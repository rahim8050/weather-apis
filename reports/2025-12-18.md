# Daily Report — 2025-12-18 (EAT)

## Done

* Locked in the **provider abstraction plan** for the weather subsystem: stable endpoints, swappable provider engines, and a canonical response schema (so providers can be swapped without changing URLs).
* Selected the **dataset roles**:

  * **Open-Meteo** for forecast + archive (historical).
  * **NASA POWER** for agro-meteorology time series (daily point data).
* Defined the **caching strategy by dataset** (TTL table) and agreed caching lives in the **service layer** (not views) so every provider benefits.
* Planned a **cache key strategy** (provider + dataset kind + normalized request payload; lat/lon rounding; hashed keys).
* Added operational guardrails: **negative caching** for transient upstream failures (timeouts/429/5xx), and discussed “stale-while-revalidate” for dashboards.
* Flagged **attribution/licensing hygiene** as part of the API contract (surface attribution in `meta` and docs).

## In progress

* Provider engine wiring plan (Open-Meteo + NASA POWER) into one `WeatherService` that normalizes output.
* Mapping strategy for provider-specific variables into your internal variable names (to avoid coupling clients to any provider).

## Next (Tomorrow — hammer time)

* Implement provider engines:

  * `weather/engines/open_meteo.py` (forecast + archive)
  * `weather/engines/nasa_power.py` (daily agro time series)
* Implement caching module + settings-based TTLs:

  * `WEATHER_CACHE_TTLS_SECONDS` + `WEATHER_CACHE_KEY_PREFIX` + canonical cache key builder
  * Apply caching in service methods per dataset kind (hourly/daily/historical/climatology) + negative caching
* Add strict request validation (serializers/validators): lat/lon ranges, date ranges, supported variables, timezone handling.
* Add tests (mock HTTP): cache-hit behavior, cache-key stability, provider mapping correctness, error/negative-cache behavior.
* Update API docs (drf-spectacular): provider support, dataset behaviors, attribution notes, and caching semantics.

## Blockers / Risks

* **Capability mismatch** between providers (some variables may not exist everywhere) → need graceful degradation rules (omit + explain, fallback provider, or 400).
* **Cache growth** with many farms + many variable combinations → may require “hot cache” + DB materialization for long history later.
* **Upstream usage constraints** (rate limits / non-commercial constraints) → keep an eye on how your deployment will be used.

## Links

* Repo: [https://github.com/rahim8050/weather-apis](https://github.com/rahim8050/weather-apis)
* PR(s): —
* Commit(s): —
* Issue(s): —
